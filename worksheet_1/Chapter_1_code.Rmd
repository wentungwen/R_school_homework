---
title: "Chapter_1"
output: word_document
---

### --------------------------------------------------------------
### AUTOMATED DATA COLLECTION WITH R
### SIMON MUNZERT, CHRISTIAN RUBBA, PETER MEISSNER, DOMINIC NYHUIS
###
### CODE CHAPTER 1: INTRODUCTION
### --------------------------------------------------------------

As you all might have noticed, the code snippets of the Chapters in Automated Data Collection With R might give you some errors. Therefore, we decided to update the chapters to fix and update the parts that lead to issues.

First off, at the end of Worksheet_1 you might remember a brief discussion on packages that give you extra functions. That is what we are going to use in this chapter and in the Worksheets next week. But the packages that are being used might need to be installed first. This can be done using the code snippet below. We are also going to use an extra package ("httr"). Why we're using this extra package is going to be explained along the way.

```{installing the packages}
install.packages("stringr")
install.packages("XML")
install.packages("maps")
install.packages("httr")
```

Alrighty, now all the relevant packages should be installed. This means that we can load the packages we just installed:

```{load packages}
library(stringr)
library(XML)
library(maps)
library(httr)
```

Next, we will parse the information from Wikipedia as described in the chapter. However, if your PC is behaving similarly like mine, it has trouble with the first line of code. It gives me the following error: "Error: failed to load external entity "http://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger". Why this is happening exactly is a mystery. But it seems that the htmlParse function is trying to access the internet in a weird way. So this is where the httr package comes in. We are going to run a slightly different code here:

```{Parse HTML from Wikipedia}
# parsing from Wikipedia web site
heritage_parsed <- GET("http://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger")
tables <- readHTMLTable(rawToChar(heritage_parsed$content))
```

You can see that the code has changed a bit, we are using the GET() function rather than htmlParse. Subsequently, we are reading the HTML table a bit differently now. But that's fine. It still achieves the same thing. The output is a tiny bit different (but that is going to be fixed in the next code chunk) With programming, you should remember that there is more than one way to skin a cat. Now that we've got the HTML code, we can get the table that we want. Or, almost at least:

```{Get the danger table}
# extract desired table
danger_table <- tables[[2]]
names(danger_table)

```

The names of our "danger_table" variable looks a bit different. The different way of parsing we did before, has resulted in a tiny difference: it doesn't recognize the names of the columns, but has put all the column names in row 1. You can see this when you click on "danger_table" in the *Environment* window on the upper right side of the screen. This shows you a nice spreadsheet of what our table looks like. Let's fix the column issue:

```{Fix the column names}
#This line names the column names after the first row in our table
colnames(danger_table) <- danger_table[1,]
#And then removes this first row from the table (those were the names anyway, so we have captured them)
danger_table <- danger_table[-1, ]
names(danger_table)
```

That is starting to look more like the code in the book! Again, the slightly different way of parsing has lead to the names being a bit better than the names in the example. That's the difference between the parsing functions, and that's totally fine. Now we finally got a table that is the same as in the book, so we can follow the code:

```{Further parse the table}
danger_table <- danger_table[, c(1, 3, 4, 6, 7)]
colnames(danger_table) <- c("name", "locn", "crit", "yins", "yend")
danger_table$name[1:3]
```

If all went well, this will look the same as the book. Now, we will cleanse the data like the book does.

```{Cleanse criteria}
# cleanse criteria
danger_table$crit <- ifelse(str_detect(danger_table$crit, "Natural")==T, "nat", "cult")
danger_table$crit[1:3]
```

The regular expression for the end year in the book seems to have an error, so I've changed it a bit:

```{Cleanse years}
# cleanse years
danger_table$yins <- as.numeric(danger_table$yins)
danger_table$yins[1:3]
danger_table$yend
yend_clean <- str_match(danger_table$yend, "(\\d{4})â€“$")[,2]
yend_clean
danger_table$yend <- as.numeric(yend_clean)
danger_table$yend[1:3]
```

Now we will get the coordinates like in the book:

```{Get coordinates}
# get coordinates
reg_y <- "[/][ -]*[[:digit:]]*[.]*[[:digit:]]*[;]"
reg_x <- "[;][ -]*[[:digit:]]*[.]*[[:digit:]]*"
y_coords <- str_extract(danger_table$locn, reg_y)
(y_coords <- as.numeric(str_sub(y_coords, 3, -2)))
danger_table$y_coords <- y_coords
x_coords <- str_extract(danger_table$locn, reg_x)
(x_coords <- as.numeric(str_sub(x_coords, 3, -1)))
danger_table$x_coords <- x_coords
danger_table$locn <- NULL

round(danger_table$y_coords, 2)[1:3]
round(danger_table$x_coords, 2)[1:3]
```

Let's check what our table looks like:

```{Check table}
dim(danger_table)
head(danger_table)
```

As you notice, the number of rows is not the same: the table has been changed since the time the book was published. This is also something to keep in mind when working with online data: websites are constantly changing. Not only the content of websites, but the HTML code that is behind the website that we see. Companies like Facebook and Instagram purposefully tweak some details in their HTML code every once in a while to make it more difficult to collect information from their website.

Now it's time to look at the coordinates on the map:

```{Create plot}
pch <- ifelse(danger_table$crit == "nat", 19, 2)
map("world", col = "darkgrey", lwd = .5, mar = c(0.1,0.1,0.1,0.1))
points(danger_table$x_coords, danger_table$y_coords, pch = pch, col = "black", cex = .8)
box()
```

And we can also create a table and a graph like they do in the book:

```{Table and graph}
table(danger_table$crit)

hist(danger_table$yend, freq=TRUE, xlab="Year when site was put on the list of endangered sites", main="")
box()
```

And the second plot:

```{Duration plot}
duration <- danger_table$yend - danger_table$yins
hist(duration, freq=TRUE, xlab="Years it took to become an endangered site", main="")
box()
```

These final plots do illustrate what you want to do with collected data. These types of data lend themselves very well for analyses. While you should always keep this in mind when collecting data during the course, it is not going to be the main focus of the course. If you want to know more about this, Chapter 10 and the case studies in part 3 of the book will provide you with plenty of examples what type of analyses are possible. This course will focus on the collection of the online data (what's in a name), although we may give some code snippets and non-mandatory bonus assignments at the end of worksheets should you be interested in this.